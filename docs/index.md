# ğŸ¤– Local LLM Agent with FastAPI + LangChain + Ollama

This repo showcases a lightweight, privacy-focused AI agent framework powered by **FastAPI**, **LangChain**, and **Ollama**, running locally with `llama3.1:8b`. It uses guardrails, embeddings, a local vector store (FAISS), and streaming via SSE to simulate "reasoning" without relying on external APIs.

> âœ… **Goal**: Showcase the abilities of small LLMs on modest hardware, while keeping the system scalable, extensible, cost-effective, and private-by-default.

---

### ğŸ§  Why LangChain?

Used the **LangChain** library due to its rich ecosystem and integrations with tools like:

* **LangGraph** for orchestrating agent flows
* **LangSmith** for observability and evaluation

These tools support a complete **AIOps lifecycle**, from prototyping to scaling in production.

---

## ğŸ”§ How to Install & Run

There are two ways to get started â€” either using Docker (recommended) or manually with Python.

### âœ… Option 1: Run with Docker (Recommended)

This spins up both the FastAPI app and the `llama3.1:8b` model (via Ollama) in a shared Docker network.

```bash
git clone https://github.com/yourname/yourrepo.git
cd yourrepo
docker-compose up --build
```

* Access the API: [http://localhost:8000/docs](http://localhost:8000/docs)
* Ollama runs at: `http://localhost:11434`

---

### ğŸ Option 2: Run Locally with Python

Use this if you want full control or prefer not to use Docker.

1. **Install dependencies**

```bash
ollama run llama3:8b  # Pulls and warms up the model
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

2. **Run the FastAPI server**

```bash
uvicorn app.main:app --reload
```

---

## ğŸ“¦ Project Structure

```bash
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI entrypoint
â”‚   â”œâ”€â”€ prompts/             # Prompt templates and chains
â”‚   â”œâ”€â”€ utils/               # Guardrails, filters, feedback
â”‚   â”œâ”€â”€ vectorstore/         # FAISS + embedding setup
â”‚   â””â”€â”€ tests/               # Unit tests
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§  Features Overview

Using LangChainâ€™s wrappers for HuggingFace and Ollama:

1. **LLM**: Installed and configured `ollama` locally. Selected `llama3.1:8b` for its performance-to-token ratio.
2. **Embeddings**: Used **Nomic Embed** via HuggingFace with remote inference for speed.
3. **Reasoning**: Since `llama3.1:8b` doesnâ€™t expose step-by-step reasoning like GPT or Claude, simulated thought processes using structured prompts + if/else logic.
4. **Guardrails**: Skips unproductive requests (e.g., greetings) with predefined responses.
5. **Vector DB**: Used **FAISS** for in-memory semantic and metadata filtering. Itâ€™s lightweight, fast, and ideal for local setups.
6. **API Server**: FastAPI streams thoughts and answers using **Server-Sent Events (SSE)**.
7. **Evaluation**: Logged performance metrics with optional LangChain evaluation hooks.
8. **Feedback Loop**: Basic thumbs-up/down system to log and improve responses.
9. **UX Prompts**: Prompts are designed to guide and engage the user proactively.
10. **Unit Tests**: Tested key utilities like filters and response validators.
11. **UI Notes**: Initially used GPT-generated UI, realized itâ€™s faster to learn a frontend framework! ğŸ˜‚ For production, tools like **Chainlit** or **OpenWebUI** are better choices.

---

## ğŸ³ Docker Overview

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY app/ ./app

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: "3.9"

services:
  fastapi-app:
    build: .
    ports:
      - "8000:8000"
    networks:
      - ollama-network
    depends_on:
      - ollama

  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    networks:
      - ollama-network
    volumes:
      - ollama-data:/root/.ollama
    restart: always

networks:
  ollama-network:

volumes:
  ollama-data:
```

---

## ğŸ“š Documentation

GitHub Pages version available here:
ğŸ‘‰ [**View Online Docs**](https://<your-username>.github.io/<your-repo-name>/)

---

## ğŸ” Possible Improvements

* [ ] Frontend using React, Svelte, or Chainlit
* [ ] Logging with LangSmith or OpenTelemetry
* [ ] Use LangGraph for reasoning trace visualization
* [ ] Add streaming feedback metrics dashboard
* [ ] Replace FAISS with Chroma or Weaviate for more scale

---

## ğŸ™Œ Acknowledgements

* LangChain team for tooling and guidance
* Ollama for enabling local LLMs with a one-liner
* HuggingFace for open embeddings and models

---

## ğŸ“„ License

MIT â€“ see [`LICENSE`](LICENSE)

